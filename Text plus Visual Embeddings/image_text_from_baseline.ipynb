{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhitabrat/MAMI/blob/main/Text%20plus%20Visual%20Embeddings/image_text_from_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEbvOBvNzSJP"
      },
      "source": [
        "Generate baseline for image plus text data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6_QQWKnczWhi",
        "outputId": "f45e8b11-022b-46d5-a537-c7055adcb122",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "requires the same folder:\n",
        "- script evaluation\n",
        "-folder 'ref' with truth.txt\n",
        "-folder 'TRAINING' with images\n",
        "'''\n",
        "\n",
        "# import evaluation\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras import regularizers\n",
        "import os\n",
        "import gc\n",
        "import shutil \n"
      ],
      "metadata": {
        "id": "uTC8UnjAzX0S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paths\n",
        "csv_path_test = '/content/drive/MyDrive/PSU/NLP Lab/MAMI/TEST/CSV/Test.csv'\n",
        "csv_path_train = '/content/drive/MyDrive/PSU/NLP Lab/MAMI/TRAIN/CSVs/training_original.csv'\n",
        "image_path = '/content/drive/MyDrive/PSU/NLP Lab/MAMI/TRAIN/Images'\n",
        "\n",
        "\n",
        "def loadImage(image_path):\n",
        "    try:\n",
        "        return load_img(image_path, target_size=(image_size, image_size))\n",
        "    except:\n",
        "        image_path = image_path.replace('png', 'jpg')\n",
        "        return load_img(image_path, target_size=(image_size, image_size))\n",
        "\n",
        "# if not os.path.exists('./ImageTextModel'):\n",
        "    # os.makedirs('./ImageTextModel')\n",
        "\n",
        "if not os.path.exists('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel'):\n",
        "    os.makedirs('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel')\n",
        "    \n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "image_size = 224\n",
        "embed_size = 768 #according to USE\n",
        "threshold = 0.5\n",
        "\n",
        "#Universal Sentence Encoder\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\"\n",
        "# module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4?tf-hub-format=compressed\"\n",
        "embed = hub.Module(module_url)"
      ],
      "metadata": {
        "id": "6biQ6eIzzeCs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#_______________________________Load Train Data_______________________________\n",
        "train_df = pd.read_csv(csv_path_train, usecols=['file_name', 'misogynous', 'Text Transcription'], sep='\\t')\n",
        "path = image_path+'/'\n",
        "\n",
        "train_df['image_path'] = path + train_df['file_name']\n",
        "\n",
        "# Universal Sentence Encoder (USE)\n",
        "'''\n",
        "Split the dataset to avoid hitting the USE call limit\n",
        "an error occurs if the 47900 steps are reached\n",
        "'''\n",
        "dfs = np.array_split(train_df, 10)\n",
        "train_df['USE'] = None\n",
        "text_embeddings=[]\n",
        "with tf.compat.v1.Session() as session:\n",
        "    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
        "   \n",
        "    for x in dfs:\n",
        "      x_data_matches = pd.Series([])\n",
        "      text_embedding = session.run(embed(list(x['Text Transcription'])))\n",
        "      text_embeddings = text_embeddings + np.array(text_embedding).tolist()\n",
        "      \n",
        "train_df['USE'] = text_embeddings\n",
        "\n",
        "#load images\n",
        "train_df['image'] = None\n",
        "train_df['image'] = train_df['image_path'].apply(lambda x: img_to_array(loadImage(x)))        \n",
        "\n",
        "#division and processing of data as input to the model\n",
        "X_train = train_df[['file_name', 'USE', 'image']]\n",
        "y_train = train_df['misogynous']\n",
        "\n",
        "#text\n",
        "tmp = []\n",
        "for value in X_train['USE']:\n",
        "    tmp.append([value])  \n",
        "tX_train = np.array(tmp)\n",
        "\n",
        "#images\n",
        "tmp = []\n",
        "for value in X_train['image']:\n",
        "  tmp.append(value) \n",
        "iX_train = np.array(tmp)\n",
        "\n",
        "#misogynous label\n",
        "tmp = []\n",
        "for value in y_train:\n",
        "    tmp.append([value])  \n",
        "y_train = np.array(tmp)\n",
        "\n",
        "#clear memory\n",
        "del train_df\n",
        "del dfs\n",
        "del text_embedding\n",
        "del text_embeddings\n",
        "\n",
        "gc.collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "83cnwG2BzeEh",
        "outputId": "b4f4d2c8-a550-4b5c-a2c2-f3f89c8b8a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f58021e03184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mx_data_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text Transcription'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m           with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m    810\u001b[0m               self._compute_dtype_object):\n\u001b[0;32m--> 811\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# or else Keras' global `learning_phase`, which might actually be a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1705\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m     \"\"\"\n\u001b[0;32m-> 1707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[0;32m--> 249\u001b[0;31m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1769\u001b[0m       if not isinstance(\n\u001b[1;32m   1770\u001b[0m           arg, (ops.Tensor, resource_variable_ops.BaseResourceVariable)):\n\u001b[0;32m-> 1771\u001b[0;31m         raise TypeError(f\"{self._flat_signature_summary()}: expected argument \"\n\u001b[0m\u001b[1;32m   1772\u001b[0m                         \u001b[0;34mf\"#{i}(zero-based) to be a Tensor; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m                         f\"got {type(arg).__name__} ({arg}).\")\n",
            "\u001b[0;31mTypeError\u001b[0m: pruned(text): expected argument #0(zero-based) to be a Tensor; got list (['Milk Milk.zip', \"ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T SAY YES, I'LL JUST RAPE YOU quickmeme.com\", 'BREAKING NEWS: Russia releases photo of DONALD TRUMP with hooker in Russian hotel.....wait....sorry....wrong file....never mind.', 'MAN SEEKING WOMAN Ignad 18 O', \"Me explaining the deep lore of. J.R.R. Tolkein's world of Arda The prostitute I am paying to keep me company during COVID quarantine 61\", \"PICTOPHLE APP *Straight white malle starts talking *Puts headphones in â˜† ears Feminism â™¡1 Reply SWIPE UP TO VIEW 5 REPLIES Chat agtinkta mah wwiem 5.0-1 Rated Chats 2 mi away RATE THIS CHAT Today, 14:29 The headphones invented by a while male? Don't ever fucking talk to me You mean on this smartphone app that was created by a white man? Or on your phone in general (that was created by a white man)? I'm gonna call the police if you don't stop harassing me Oh ok, you mean get a white male to come help you? GIF Say something nice... DESTRUCTION 100 SEND\", 'Chinese restaurant faces closure after 30 years following complaints from leading judge, 64, and wife over cooking smells wafting into their Â£525,000 home they bought three years ago', 'YOU WERE THE CHOSEN ONE! YOU WERE MEANT TO DESTROY THE DOWNVOTERS, NOT JOIN THEM! imgflip.com', '* 84% 6:07 PM Pull start for dishwasher $19 Listed 22 hours ago Send seller a message Is this available? Send ifunny.co', 'SCIENTISTS HAVE DIS..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________IMAGE MODEL_______________________________\n",
        "l2_strength = 1e-5\n",
        "\n",
        "input_image = layers.Input(shape=(image_size,image_size,3))\n",
        "vgg_model = VGG16(input_tensor = input_image, weights = 'imagenet', include_top=False)\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = vgg_model.output\n",
        "x = layers.Flatten(input_shape=vgg_model.output_shape[1:])(x)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_strength))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_strength))(x)\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "image_model = Model(vgg_model.input, x)\n",
        "\n",
        "image_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "image_model.summary()\n",
        "\n",
        "history = image_model.fit(iX_train, \n",
        "                    y_train,\n",
        "                    validation_split=0.1,\n",
        "                    epochs= 30,\n",
        "                    batch_size=batch_size,\n",
        "                    #verbose=0,\n",
        "                    )\n",
        "                    \n",
        "# image_model.save('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/image_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "NSthHOR5zeGt",
        "outputId": "c4580175-e301-40ed-b04a-b5ba3dcf9721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               6422784   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,153,985\n",
            "Trainable params: 6,439,297\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/30\n",
            "9000/9000 [==============================] - ETA: 0s - loss: 3.4260 - accuracy: 0.5660"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9000/9000 [==============================] - 35s 4ms/sample - loss: 3.4260 - accuracy: 0.5660 - val_loss: 0.6776 - val_accuracy: 0.5640\n",
            "Epoch 2/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.8286 - accuracy: 0.5932 - val_loss: 0.6623 - val_accuracy: 0.6600\n",
            "Epoch 3/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.6957 - accuracy: 0.6408 - val_loss: 0.7802 - val_accuracy: 0.6550\n",
            "Epoch 4/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.7171 - accuracy: 0.6546 - val_loss: 0.6298 - val_accuracy: 0.6690\n",
            "Epoch 5/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.7090 - accuracy: 0.6609 - val_loss: 0.6176 - val_accuracy: 0.6930\n",
            "Epoch 6/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.6525 - accuracy: 0.6777 - val_loss: 0.6475 - val_accuracy: 0.6630\n",
            "Epoch 7/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.6487 - accuracy: 0.6873 - val_loss: 0.6131 - val_accuracy: 0.6930\n",
            "Epoch 8/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.6181 - accuracy: 0.6920 - val_loss: 0.6456 - val_accuracy: 0.6820\n",
            "Epoch 9/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5731 - accuracy: 0.6887 - val_loss: 0.7005 - val_accuracy: 0.6980\n",
            "Epoch 10/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5997 - accuracy: 0.7066 - val_loss: 0.6760 - val_accuracy: 0.6800\n",
            "Epoch 11/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5615 - accuracy: 0.7087 - val_loss: 0.6605 - val_accuracy: 0.6960\n",
            "Epoch 12/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5642 - accuracy: 0.7082 - val_loss: 0.6875 - val_accuracy: 0.7060\n",
            "Epoch 13/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5453 - accuracy: 0.7154 - val_loss: 0.7191 - val_accuracy: 0.6890\n",
            "Epoch 14/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5472 - accuracy: 0.7176 - val_loss: 0.6455 - val_accuracy: 0.6940\n",
            "Epoch 15/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5177 - accuracy: 0.7248 - val_loss: 0.7247 - val_accuracy: 0.6920\n",
            "Epoch 16/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5217 - accuracy: 0.7236 - val_loss: 0.8048 - val_accuracy: 0.6900\n",
            "Epoch 17/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5339 - accuracy: 0.7242 - val_loss: 0.7228 - val_accuracy: 0.6940\n",
            "Epoch 18/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5221 - accuracy: 0.7296 - val_loss: 0.7812 - val_accuracy: 0.6970\n",
            "Epoch 19/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5204 - accuracy: 0.7248 - val_loss: 0.7462 - val_accuracy: 0.6810\n",
            "Epoch 20/30\n",
            "9000/9000 [==============================] - 22s 2ms/sample - loss: 0.5083 - accuracy: 0.7320 - val_loss: 0.9022 - val_accuracy: 0.6840\n",
            "Epoch 21/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5291 - accuracy: 0.7363 - val_loss: 0.7440 - val_accuracy: 0.7020\n",
            "Epoch 22/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5248 - accuracy: 0.7433 - val_loss: 0.8131 - val_accuracy: 0.6970\n",
            "Epoch 23/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5164 - accuracy: 0.7360 - val_loss: 0.7551 - val_accuracy: 0.7060\n",
            "Epoch 24/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.4847 - accuracy: 0.7459 - val_loss: 0.6840 - val_accuracy: 0.7010\n",
            "Epoch 25/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.4917 - accuracy: 0.7514 - val_loss: 0.8444 - val_accuracy: 0.7070\n",
            "Epoch 26/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5048 - accuracy: 0.7442 - val_loss: 0.8082 - val_accuracy: 0.6950\n",
            "Epoch 27/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5009 - accuracy: 0.7537 - val_loss: 0.8954 - val_accuracy: 0.7000\n",
            "Epoch 28/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.5026 - accuracy: 0.7494 - val_loss: 0.8872 - val_accuracy: 0.7070\n",
            "Epoch 29/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.4796 - accuracy: 0.7676 - val_loss: 0.8034 - val_accuracy: 0.7100\n",
            "Epoch 30/30\n",
            "9000/9000 [==============================] - 21s 2ms/sample - loss: 0.4905 - accuracy: 0.7540 - val_loss: 0.9609 - val_accuracy: 0.7040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________TEXT MODEL_______________________________\n",
        "input_text = layers.Input(shape=(1, embed_size))\n",
        "l = layers.Dense(1, activation='sigmoid')(input_text)\n",
        "text_model = Model(inputs=[input_text], outputs=l)\n",
        "text_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "text_model.summary()\n",
        "\n",
        "history = text_model.fit(tX_train, \n",
        "                    y_train,\n",
        "                    validation_split=0.1,\n",
        "                    epochs= 30,\n",
        "                    batch_size=batch_size,\n",
        "                    #verbose=0\n",
        "                    )\n",
        "\n",
        "text_model.save('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/text_model.h5')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Qz9JD9DzeJx",
        "outputId": "4dd701ad-14d9-4391-e258-ac06adbd2485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1, 512)]          0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1, 1)              513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 513\n",
            "Trainable params: 513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/30\n",
            "8896/9000 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5027"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9000/9000 [==============================] - 4s 453us/sample - loss: 0.6933 - accuracy: 0.5027 - val_loss: 0.6941 - val_accuracy: 0.4833\n",
            "Epoch 2/30\n",
            "9000/9000 [==============================] - 1s 69us/sample - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6945 - val_accuracy: 0.4816\n",
            "Epoch 3/30\n",
            "9000/9000 [==============================] - 1s 66us/sample - loss: 0.6933 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5038\n",
            "Epoch 4/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6932 - accuracy: 0.5041 - val_loss: 0.6939 - val_accuracy: 0.4903\n",
            "Epoch 5/30\n",
            "9000/9000 [==============================] - 1s 63us/sample - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6955 - val_accuracy: 0.4794\n",
            "Epoch 6/30\n",
            "9000/9000 [==============================] - 1s 62us/sample - loss: 0.6933 - accuracy: 0.5028 - val_loss: 0.6938 - val_accuracy: 0.4895\n",
            "Epoch 7/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6932 - accuracy: 0.5053 - val_loss: 0.6942 - val_accuracy: 0.4904\n",
            "Epoch 8/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6945 - val_accuracy: 0.4860\n",
            "Epoch 9/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6933 - accuracy: 0.5039 - val_loss: 0.6936 - val_accuracy: 0.4945\n",
            "Epoch 10/30\n",
            "9000/9000 [==============================] - 1s 61us/sample - loss: 0.6931 - accuracy: 0.5054 - val_loss: 0.6962 - val_accuracy: 0.4740\n",
            "Epoch 11/30\n",
            "9000/9000 [==============================] - 1s 63us/sample - loss: 0.6932 - accuracy: 0.5061 - val_loss: 0.6966 - val_accuracy: 0.4723\n",
            "Epoch 12/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6933 - accuracy: 0.5047 - val_loss: 0.6948 - val_accuracy: 0.4839\n",
            "Epoch 13/30\n",
            "9000/9000 [==============================] - 1s 63us/sample - loss: 0.6932 - accuracy: 0.5061 - val_loss: 0.6931 - val_accuracy: 0.5060\n",
            "Epoch 14/30\n",
            "9000/9000 [==============================] - 1s 64us/sample - loss: 0.6932 - accuracy: 0.5066 - val_loss: 0.6949 - val_accuracy: 0.4816\n",
            "Epoch 15/30\n",
            "9000/9000 [==============================] - 1s 64us/sample - loss: 0.6932 - accuracy: 0.5062 - val_loss: 0.6935 - val_accuracy: 0.4976\n",
            "Epoch 16/30\n",
            "9000/9000 [==============================] - 1s 64us/sample - loss: 0.6932 - accuracy: 0.5051 - val_loss: 0.6932 - val_accuracy: 0.5004\n",
            "Epoch 17/30\n",
            "9000/9000 [==============================] - 1s 61us/sample - loss: 0.6931 - accuracy: 0.5053 - val_loss: 0.6945 - val_accuracy: 0.4861\n",
            "Epoch 18/30\n",
            "9000/9000 [==============================] - 1s 62us/sample - loss: 0.6933 - accuracy: 0.5038 - val_loss: 0.6944 - val_accuracy: 0.4851\n",
            "Epoch 19/30\n",
            "9000/9000 [==============================] - 1s 61us/sample - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6944 - val_accuracy: 0.4842\n",
            "Epoch 20/30\n",
            "9000/9000 [==============================] - 1s 62us/sample - loss: 0.6932 - accuracy: 0.5055 - val_loss: 0.6957 - val_accuracy: 0.4747\n",
            "Epoch 21/30\n",
            "9000/9000 [==============================] - 1s 64us/sample - loss: 0.6932 - accuracy: 0.5046 - val_loss: 0.6923 - val_accuracy: 0.5209\n",
            "Epoch 22/30\n",
            "9000/9000 [==============================] - 1s 65us/sample - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6920 - val_accuracy: 0.5219\n",
            "Epoch 23/30\n",
            "9000/9000 [==============================] - 1s 65us/sample - loss: 0.6934 - accuracy: 0.5028 - val_loss: 0.6922 - val_accuracy: 0.5231\n",
            "Epoch 24/30\n",
            "9000/9000 [==============================] - 1s 66us/sample - loss: 0.6932 - accuracy: 0.5065 - val_loss: 0.6942 - val_accuracy: 0.4908\n",
            "Epoch 25/30\n",
            "9000/9000 [==============================] - 1s 62us/sample - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6955 - val_accuracy: 0.4750\n",
            "Epoch 26/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6956 - val_accuracy: 0.4789\n",
            "Epoch 27/30\n",
            "9000/9000 [==============================] - 1s 63us/sample - loss: 0.6931 - accuracy: 0.5066 - val_loss: 0.6945 - val_accuracy: 0.4808\n",
            "Epoch 28/30\n",
            "9000/9000 [==============================] - 1s 70us/sample - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6949 - val_accuracy: 0.4833\n",
            "Epoch 29/30\n",
            "9000/9000 [==============================] - 1s 60us/sample - loss: 0.6933 - accuracy: 0.5056 - val_loss: 0.6918 - val_accuracy: 0.5265\n",
            "Epoch 30/30\n",
            "9000/9000 [==============================] - 1s 63us/sample - loss: 0.6933 - accuracy: 0.5039 - val_loss: 0.6941 - val_accuracy: 0.4867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = image_model.layers[len(image_model.layers)-2].output\n",
        "reshape = layers.Reshape((1, image_model.layers[len(image_model.layers)-2].output_shape[1]), name='predictions')(image)\n",
        "text = text_model.layers[0].output\n",
        "\n",
        "input = tf.keras.layers.Concatenate(axis=-1)([text, reshape])\n",
        "\n",
        "l = layers.Dense(1, activation='sigmoid')(input)\n",
        "model = Model(inputs=[input_text, input_image], outputs=[l])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YUHcHY98SWHL",
        "outputId": "718ad2cc-81e8-40f9-cad9-771ec02481d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)     (None, 7, 7, 512)    0           ['block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 25088)        0           ['block5_pool[0][0]']            \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 256)          6422784     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256)          0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 64)           16448       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1, 512)]     0           []                               \n",
            "                                                                                                  \n",
            " predictions (Reshape)          (None, 1, 64)        0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 1, 576)       0           ['input_2[0][0]',                \n",
            "                                                                  'predictions[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 1, 1)         577         ['concatenate_9[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,154,497\n",
            "Trainable params: 6,439,809\n",
            "Non-trainable params: 14,714,688\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "nJoDeCowSWTu",
        "outputId": "a0b21f9e-6b7f-41f3-bda7-e73bb83b61fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_2:0' shape=(None, 1, 512) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshape"
      ],
      "metadata": {
        "id": "nat5Ui9PShWw",
        "outputId": "5318b044-a3f0-4c74-e9ca-79710bb0abc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'predictions/Reshape:0' shape=(None, 1, 64) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text"
      ],
      "metadata": {
        "id": "LZY31XC9Shao",
        "outputId": "f0707551-9ce9-4ee0-bd00-e0cbd095302c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_2:0' shape=(None, 1, 512) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image"
      ],
      "metadata": {
        "id": "KcRxce7ZShft",
        "outputId": "e93cf048-5a08-4e7d-95e5-570fdbfcbd5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "PBabrJepShjJ",
        "outputId": "119365ce-5852-4639-9126-cc5d4ab5083f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense_12/Sigmoid:0' shape=(None, 1, 1) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "drQ40n67Shk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xxJoF1DRShoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g_t3z4A1Shq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_text"
      ],
      "metadata": {
        "id": "fkIj-UzSqRle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text\n",
        "\n",
        "# downloading the pre-trained BERT model from tfhub\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
      ],
      "metadata": {
        "id": "QQDiSrTPoGF-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing BERT layers\n",
        "text_input_bert = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input_text') # input layer\n",
        "preprocessed_text = bert_preprocess(text_input_bert)\n",
        "outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "reshape = tf.keras.layers.Reshape(target_shape=(1, 768))(outputs['default'])\n",
        "\n",
        "# initializing NN layer\n",
        "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(reshape)\n",
        "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "\n",
        "text_model_bert = Model(inputs=[text_input_bert], outputs=l)\n",
        "text_model_bert.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "text_model_bert.summary()"
      ],
      "metadata": {
        "id": "cr5LHkl7oKXI",
        "outputId": "2f85495b-27f1-4979-9811-37d942b3858e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_text (InputLayer)        [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_2 (KerasLayer)     {'input_mask': (Non  0           ['input_text[0][0]']             \n",
            "                                e, 128),                                                          \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " keras_layer_3 (KerasLayer)     {'encoder_outputs':  109482241   ['keras_layer_2[0][0]',          \n",
            "                                 [(None, 128, 768),               'keras_layer_2[0][1]',          \n",
            "                                 (None, 128, 768),                'keras_layer_2[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768)}                                                \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 1, 768)       0           ['keras_layer_3[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1, 768)       0           ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1, 1)         769         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv(csv_path_train, usecols=['file_name', 'misogynous', 'Text Transcription'], sep='\\t')"
      ],
      "metadata": {
        "id": "w_tPx7ymoKwH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = text_model.fit(df_1['Text Transcription'],\n",
        "                         df_1['misogynous'],\n",
        "                         validation_split=0.1,\n",
        "                         epochs= 10,\n",
        "                         batch_size=batch_size,\n",
        "                         #verbose=0\n",
        "                         )"
      ],
      "metadata": {
        "id": "ebgdqe-C5jD0",
        "outputId": "17e2e85f-3cba-4477-c688-8ce9768af6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "282/282 [==============================] - 72s 223ms/step - loss: 0.6821 - accuracy: 0.5636 - val_loss: 0.6581 - val_accuracy: 0.6140\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6570 - accuracy: 0.6137 - val_loss: 0.6493 - val_accuracy: 0.6110\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6446 - accuracy: 0.6294 - val_loss: 0.6370 - val_accuracy: 0.6610\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6333 - accuracy: 0.6468 - val_loss: 0.6269 - val_accuracy: 0.6710\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6228 - accuracy: 0.6617 - val_loss: 0.6225 - val_accuracy: 0.6740\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6183 - accuracy: 0.6676 - val_loss: 0.6218 - val_accuracy: 0.6720\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6059 - accuracy: 0.6786 - val_loss: 0.6307 - val_accuracy: 0.6510\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6104 - accuracy: 0.6742 - val_loss: 0.6198 - val_accuracy: 0.6650\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.6024 - accuracy: 0.6806 - val_loss: 0.6106 - val_accuracy: 0.6880\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 61s 218ms/step - loss: 0.5985 - accuracy: 0.6847 - val_loss: 0.6080 - val_accuracy: 0.6940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model.layers[5].output"
      ],
      "metadata": {
        "id": "_BfyHa5g9kR0",
        "outputId": "e170b147-7ead-4dc2-acbc-a2a3c53c62ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1, 1) dtype=float32 (created by layer 'output')>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-uI-t1CD9kUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "84yJe3lm9kWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_image = layers.Input(shape=(image_size,image_size,3))\n",
        "# vgg_model = VGG16(input_tensor = input_image, weights = 'imagenet', include_top=False)"
      ],
      "metadata": {
        "id": "s42i2B5J9kZV",
        "outputId": "14c7e239-a811-4e7b-dec2-9d4a04eaf379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# txt_model = tf.keras.models.load_model('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/text_model.h5')\n",
        "# # txt_model.summary()\n",
        "# txt_model.layers[0].output"
      ],
      "metadata": {
        "id": "sRdqSyFW9kbV",
        "outputId": "930a591c-fa55-4089-9142-99e2bcd443a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1, 512) dtype=float32 (created by layer 'input_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # load image_model from dir\n",
        "# image_model = tf.keras.models.load_model('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/image_model.h5')\n",
        "# # image_model.summary()"
      ],
      "metadata": {
        "id": "aZG7s2z1oKzJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________IMAGE-TEXT MODEL_______________________________\n",
        "image = image_model.layers[len(image_model.layers)-2].output\n",
        "reshape = layers.Reshape((1, image_model.layers[len(image_model.layers)-2].output_shape[1]), name='predictions')(image)\n",
        "text = text_model_bert.layers[3].output\n",
        "\n",
        "input = tf.keras.layers.Concatenate(axis=-1)([text, reshape])\n",
        "\n",
        "l = layers.Dense(1, activation='sigmoid')(input)\n",
        "model = Model(inputs=[text, input_image], outputs=[l])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# model.save('/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/image_text_model.h5')\n",
        "# tf.keras.utils.plot_model(model, \"/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/model.png\", show_shapes=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "9bFHxXaozeN1",
        "outputId": "4ed807ac-572e-4496-9a93-dde857090d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_10\" was not an Input tensor, it was generated by layer \"reshape_1\".\n",
            "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
            "The tensor that caused the issue was: Tensor(\"reshape_1/Reshape:0\", shape=(None, 1, 768), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_10\" was not an Input tensor, it was generated by layer \"reshape_1\".\n",
            "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
            "The tensor that caused the issue was: Tensor(\"reshape_1/Reshape:0\", shape=(None, 1, 768), dtype=float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-e6fe749461d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 230\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;34mf'Graph disconnected: cannot obtain value for tensor {x} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m                 \u001b[0;34mf'at layer \"{layer.name}\". The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 f'were accessed without issue: {layers_with_complete_input}')\n",
            "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_text:0\", shape=(None,), dtype=string) at layer \"keras_layer_2\". The following previous layers were accessed without issue: ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_pool', 'flatten', 'dense_8', 'dropout']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_bert.layers[3].output"
      ],
      "metadata": {
        "id": "cTvV1RCIU8Y-",
        "outputId": "d46d93bd-2c85-4033-ee19-5638f6279b3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'reshape_1/Reshape:0' shape=(None, 1, 768) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "acqosR08U8ca",
        "outputId": "61253000-bc03-48b1-a408-4c7fe38368be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concatenate_10/concat:0' shape=(None, 1, 832) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(text_model_bert)"
      ],
      "metadata": {
        "id": "CnyhPTDvU8hZ",
        "outputId": "f1425981-5a3a-419a-f952-05afbafbeb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N4lajMhcU8k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RdzbDDGxU8nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________Load Test Data_______________________________\n",
        "#clear memory\n",
        "# del X_train\n",
        "# del y_train\n",
        "# del iX_train\n",
        "# del tX_train\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# paths\n",
        "csv_path_test = '/content/drive/MyDrive/PSU/NLP Lab/MAMI/TEST/CSV/Test.csv'\n",
        "image_path = '/content/drive/MyDrive/PSU/NLP Lab/MAMI/TEST/Images'\n",
        "\n",
        "#load data\n",
        "test_df = pd.read_csv(csv_path_test, sep='\\t')\n",
        "path = image_path+'/'\n",
        "test_df['image_path'] = path + test_df['file_name']\n",
        "\n",
        "# Universal Sentence Encoder (USE)\n",
        "'''\n",
        "Split the dataset to avoid hitting the USE call limit\n",
        "an error occurs if the 47900 steps are reached\n",
        "'''\n",
        "dfs = np.array_split(test_df, 10)\n",
        "test_df['USE'] = None\n",
        "text_embeddings=[]\n",
        "with tf.compat.v1.Session() as session:\n",
        "    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
        "   \n",
        "    for x in dfs:\n",
        "      x_data_matches = pd.Series([])\n",
        "      text_embedding = session.run(embed(list(x['Text Transcription'])))\n",
        "      text_embeddings = text_embeddings + np.array(text_embedding).tolist()\n",
        "test_df['USE'] = text_embeddings\n",
        "\n",
        "#Load images\n",
        "test_df['image'] = None\n",
        "test_df['image'] = test_df['image_path'].apply(lambda x: img_to_array(loadImage(x)))        \n",
        "\n",
        "#division and processing of data as input to the model\n",
        "X_test = test_df[['file_name', 'USE', 'image']]\n",
        "\n",
        "#Text\n",
        "tmp = []\n",
        "for value in X_test['USE']:\n",
        "    tmp.append([value])  \n",
        "tX_test = np.array(tmp)\n",
        "\n",
        "\n",
        "#images\n",
        "tmp = []\n",
        "for value in X_test['image']:\n",
        "    tmp.append(value)  \n",
        "iX_test = np.array(tmp)\n",
        "\n",
        "#clear memory\n",
        "del tmp\n",
        "del dfs\n",
        "del text_embedding\n",
        "del text_embeddings\n",
        "\n",
        "gc.collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "rvpHEKnlzePt",
        "outputId": "1f6c8fc8-1822-4ada-c746-1e7ef387cbb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________PREDICTION_______________________________\n",
        "predictions = model.predict([tX_test, iX_test], batch_size=batch_size)\n",
        "predictions = predictions.reshape(predictions.shape[0])\n",
        "pred = predictions > threshold\n",
        "pred = list(map(int, pred)) #true/false to 1/0\n",
        "\n",
        "predictions_db = pd.DataFrame(data=test_df['file_name'])\n",
        "predictions_db['misogynist'] = pred\n"
      ],
      "metadata": {
        "id": "WA00s39mzeSY",
        "outputId": "a62be7cf-53fc-4b21-ec81-10f041f091e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_db.to_csv(\"/content/drive/MyDrive/PSU/NLP Lab/MAMI/result.csv\", index=False)\n",
        "predictions_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "s44dE2ejlGhd",
        "outputId": "a92e35fb-ebd5-4c97-8b51-3781126bb1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6579ac63-5fd0-4d29-9f4e-781373a735da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15236.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15805.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16254.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16191.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15952.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>15591.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>15049.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>15363.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>15199.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>15853.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6579ac63-5fd0-4d29-9f4e-781373a735da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6579ac63-5fd0-4d29-9f4e-781373a735da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6579ac63-5fd0-4d29-9f4e-781373a735da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     file_name  misogynist\n",
              "0    15236.jpg           0\n",
              "1    15805.jpg           0\n",
              "2    16254.jpg           1\n",
              "3    16191.jpg           1\n",
              "4    15952.jpg           0\n",
              "..         ...         ...\n",
              "995  15591.jpg           0\n",
              "996  15049.jpg           0\n",
              "997  15363.jpg           0\n",
              "998  15199.jpg           0\n",
              "999  15853.jpg           0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluation\n",
        "\n",
        "#_______________________________EVALUATION_______________________________\n",
        "if not os.path.exists('./res'):\n",
        "    os.makedirs('./res')\n",
        "\n",
        "predictions_db.to_csv('./res/answer.txt', index=False, sep='\\t', header=False)\n",
        "evaluation.main(['','./', '/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/'])\n",
        "#move res folder to ImageTextModel folder\n",
        "shutil.move('./res/', '/content/drive/MyDrive/PSU/NLP Lab/MAMI/Saved Models/ImageTextModel/res/')\n"
      ],
      "metadata": {
        "id": "b9A5q9z5zeUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# f1_score(test_df['misogynous'], predictions_db['misogynist'], average='micro')"
      ],
      "metadata": {
        "id": "NwALGpOfzeWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KpjxNJVxzeZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eOpvAsrnzea6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OJCzqBeszefE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ADZoBJPhzehB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VTP-1hmqzekj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "image_text_baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}