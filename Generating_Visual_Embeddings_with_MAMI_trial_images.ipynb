{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Visual Embeddings with MAMI trial images",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeamMAMI/MAMI/blob/GenerateVisualEmbeddings/Generating_Visual_Embeddings_with_MAMI_trial_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qv-4b7LYOVw"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnSobfsQYTEG"
      },
      "source": [
        "*Disclaimer: This notebook is based on my understanding of the [detectron2](https://github.com/facebookresearch/detectron2) and the [visualbert](https://github.com/uclanlp/visualbert) repositories. Hence, I do not guarantee that this is the \"correct\" or \"recommended\" way to get visual embeddings from detectron2. Having said that, I'm definitely looking to improve this notebook and open to any criticism/suggestions. You can reach me at chhablani.gunjan@gmail.com with any issues that concern you regarding this notebook.*\n",
        "\n",
        "This notebook is based on the concept in the [script to extract image features](https://github.com/uclanlp/visualbert/blob/master/utils/get_image_features/extract_image_features_nlvr.py) for NLVR2 task in the [visualbert](https://github.com/uclanlp/visualbert) repository. You can refer to this script for a \"safer\" way to extract visual embeddings. The script uses [detectron](https://github.com/facebookresearch/Detectron) and it'll be fairly easier to use it (I hope) without getting into the nitty-gritty.\n",
        "\n",
        "However, for the sake of using detectron2, which will have better support (for the foreseeable future) than detectron, I present this notebook example to you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1oP3kTD4vCZ",
        "outputId": "f7f4b823-ba8b-45b4-93a0-858c18461485"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY8GzVcva0Xc"
      },
      "source": [
        "For extracting visual embeddings, we need the features from various regions in the image which are used in the classification. This means that we need to \"detect\" the regions which might have objects in them.\n",
        "\n",
        "The detectron2 library, off-the-shelf, does not support intermediate tensor extraction. But, there are ways the user can get the values of these tensors with some effort. See the docs [here](https://detectron2.readthedocs.io/en/latest/tutorials/models.html#partially-execute-a-model). In this notebook, I will be using the *partial execution* method as described in the docs. I admit that the other approaches might be easier or better suited, but this is just a start ;).\n",
        "\n",
        "**Tip:** If you're looking to play with detectron2, you might like this [Colab tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_GB-tubtJs"
      },
      "source": [
        "For the purpose of this notebook, I will be using an example from the [VQA v2](https://visualqa.org/download.html) validation set, as it is one of the tasks VisualBert has been used for. VisualBert authors used pre-generated embeddings for VQA v2, however."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-mLSDb74g58"
      },
      "source": [
        "## How it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Aa4ojk4mTx"
      },
      "source": [
        "The model checkpoint that we will be using for this notebook is a MaskRCNN+ResNet-101+FPN checkpoint.\n",
        "\n",
        "First, the image features are generated at various scales using the ResNet+FPN backbone. These features are then passed to the region proposal network or RPN. RPN generates 1000 region proposals, which are then passed to ROI Heads. ROI Heads perform the classification and box-regression and after that the predictions are aligned using ROIAlign layer and to the mask RCNN heads.\n",
        "\n",
        "We want to extract the box features in the ROI heads which are used for classification. However, we don't want to select all the proposals (as there are 1000 of them!). For the same, we use the NMS with a threshold. Then the boxes are further filtered using a class score threshold. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf-pOxtSfPF0"
      },
      "source": [
        "### Install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "643yOpAZwRWq",
        "outputId": "c9ccde64-dad1-4487-a820-9e846eb51ee9"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKj-udtwT8o",
        "outputId": "a0480875-ccc6-42c4-df39-6dbb47342ee9"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P5ZDpfKXlHX"
      },
      "source": [
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "%%capture\n",
        "!pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG9jjTSOfhX6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xACCRQgLfhLG"
      },
      "source": [
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from pandas import *\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "import torch, torchvision\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8EUjamjkT6p"
      },
      "source": [
        "from detectron2 import model_zoo\n",
        "from detectron2.layers import nms\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.layers import ShapeSpec\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.structures.boxes import Boxes\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.structures.image_list import ImageList\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLNocZRfThd"
      },
      "source": [
        "### Download the VQA v2 Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46stwSuFfpQQ"
      },
      "source": [
        "### Load Examples\n",
        "The next few cells show how to get an example from the VQA v2 dataset. We will only use the image from the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za2KDaGLtFLq",
        "outputId": "ce1df017-5d3d-478b-f683-6f7dc686bc6e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwQ3TTXotIbA",
        "outputId": "30437a4e-0a82-4eab-dfb5-7dc32a89eb57"
      },
      "source": [
        "%cd drive/Shareddrives/team_MAMI/MAMI/TRIAL\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/team_MAMI/MAMI/TRIAL\n",
            "Images\ttrial.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8WymuInuIrv"
      },
      "source": [
        "# from pandas import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va4s2RC7fuzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0748ba60-7c19-4f53-86dc-0ddaebcec4eb"
      },
      "source": [
        "# with open('v2_OpenEnded_mscoco_val2014_questions.json') as f:\n",
        "    # q = json.load(f)\n",
        "data = read_csv('trial.csv', sep='\\t')\n",
        "print(data)\n",
        "f_name = data['file_name'].tolist()\n",
        "msgyn = data['misogynous'].tolist()\n",
        "txt_trnscrpt = data['Text Transcription'].tolist()\n",
        "print(f_name)\n",
        "print(msgyn)\n",
        "print(txt_trnscrpt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   file_name  ...                                 Text Transcription\n",
            "0     28.jpg  ...  not now, dad. We should burn Jon Snow. stop it...\n",
            "1     30.jpg  ...  there may have been a mixcommunication with th...\n",
            "2     33.jpg  ...                      i shouldn't have sold my boat\n",
            "3     58.jpg  ...    Bitches be like, It was my fault i made him mad\n",
            "4     89.jpg  ...  find a picture of 4 girls together on FB make ...\n",
            "..       ...  ...                                                ...\n",
            "95  1380.jpg  ...  Rape culture.  It's what every oxymoronic femi...\n",
            "96  1381.jpg  ...  walking, running, telereporting, not going to ...\n",
            "97  1384.jpg  ...  taking the time to get her pussy wet. always p...\n",
            "98  1408.jpg  ...         what men play with vs what women play with\n",
            "99  1440.jpg  ...  Girls boys school travel sport shopping going ...\n",
            "\n",
            "[100 rows x 7 columns]\n",
            "['28.jpg', '30.jpg', '33.jpg', '58.jpg', '89.jpg', '97.jpg', '104.jpg', '122.jpg', '126.jpg', '133.jpg', '142.jpg', '156.jpg', '157.jpg', '161.jpg', '162.jpg', '165.jpg', '175.jpg', '181.jpg', '191.jpg', '207.jpg', '216.jpg', '230.jpg', '236.jpg', '246.jpg', '247.jpg', '274.jpg', '282.jpg', '300.jpg', '349.jpg', '381.jpg', '383.jpg', '415.jpg', '441.jpg', '442.jpg', '447.jpg', '448.jpg', '482.jpg', '491.jpg', '492.jpg', '519.jpg', '520.jpg', '524.jpg', '525.jpg', '565.jpg', '571.jpg', '576.jpg', '625.jpg', '643.jpg', '656.jpg', '666.jpg', '670.jpg', '700.jpg', '717.jpg', '719.jpg', '739.jpg', '741.jpg', '788.jpg', '822.jpg', '828.jpg', '840.jpg', '854.jpg', '859.jpg', '865.jpg', '872.jpg', '899.jpg', '919.jpg', '922.jpg', '925.jpg', '960.jpg', '977.jpg', '980.jpg', '981.jpg', '989.jpg', '1012.jpg', '1032.jpg', '1047.jpg', '1078.jpg', '1094.jpg', '1132.jpg', '1146.jpg', '1159.jpg', '1174.jpg', '1175.jpg', '1181.jpg', '1229.jpg', '1236.jpg', '1263.jpg', '1285.jpg', '1294.jpg', '1309.jpg', '1310.jpg', '1327.jpg', '1354.jpg', '1363.jpg', '1377.jpg', '1380.jpg', '1381.jpg', '1384.jpg', '1408.jpg', '1440.jpg']\n",
            "[0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
            "[\"not now, dad. We should burn Jon Snow. stop it dad. you know what will happen if he becomes a white walker? he'll become SNOW WHITE. SNOW WHITE!\", \"there may have been a mixcommunication with the decorator.. happy birthday dick ha ha ha don't write that his name is matt\", \"i shouldn't have sold my boat\", 'Bitches be like, It was my fault i made him mad', 'find a picture of 4 girls together on FB make a comemnt that \"you three look pretty\"', \"Circumcision. If circumcision is alright, because the child won't remember it, then is date rape alright as well?\", \"hey! i'm peeing over here\", 'what am i doing with my lives?', '\"No mere men can kill me\" \"We are no mere men\" \"What are they gonna do? Accuse him of sexual assault?\"', 'said something funny, someone else said it again only louder and people laughed', 'Hello prince! Will you rape me and live happily ever after with me?', 'when people give out broccoli on halloween', \"I have no idea what i'm doing\", 'This is like so traumatizing, a little more to left', \"in my client's defense the red light and green light look exaclty the same\", 'open birthday card, pretend not to notice money as you finish it reading it', 'Yeah I believe God is a woman. Only a woman could be so emotionally unstable.', \"we're fifa, we have to be fair. ooh loo, a rich country wants to host in 2018\", \"i just met you, and this is crazy. but i'm from the future and i'm your baby\", \"The girls who don't show attitude are actually the best\", 'coffee is ready', \"what if i told you raydong isn't an actual dog\", 'The perfect woman Exhibit A', 'Equal rights and responsibilities complaining feminists equal rights and responsibilities feminists', 'What do you call a dumb blonde with two brain cells? Pregnant', 'love is in the air? wrong. nitrogeon, oxygen anda carbon dioxide are in the air', \"I'm a woman and I have rights lol\", 'when your freind says bazzinga', \"Woman cause these samiches ain't gonna make themselves.\", 'Can a woman make you a millionaire?Yes! If you are a billionaire.', 'when u have nice hat and someone mentions it and you feel nice', 'apply cold water to the burned area', \"Women men Happy International Women's Day\", 'What if Zelda was a girl? The stupidity it hurts', '36 years ago at the party. Now she remembers Judge Kavanaugh groped her', \"you may think you're hipster but you'll never be car-seat replaced-with-vintage rocking-chair hipster\", \"Do you want to know why women don't need watches? Because there are clocks on the stove\", \"this could be us but i hate running nd you don't exist\", \"the ol' razzle dazzle\", 'excuse me sir, \"do you have time to talk about our lord, Poseidon?\"', 'hel up help you get 1000 dollars in cash', \"what if...one day google was deleted and we couldn't google what happened to google\", \"please tell me the calendar is wrong.. it can't be monday again\", 'Taylor Swift? More like...Taylor-Swift-to-be-with-every-guy', 'if mary gave birth to jesus and jesus is the lamb of god.. did mary have a little lamb?', 'Hi ho. what are you doing? \"Nothing, babe. Just women stuff\" \"You crashed the car again?\" Women…', 'internet is down me too', 'friends with benefits? what like healthcare and dental', \"Some women are single because a good man ain't their type\", \"i'm making a zero with my hand, but i say something related to one.\", \"hello from the other side i must've meowed a thousands time\", \"she said she'd call me maybe...\", 'Girls be like...look at my new bracelet', 'Never let your man leave the house hungry or horny. Because somewhere out there is a whore with sammiches.', 'i be in the 5ft area of the pool like', \"You're supposed to pull my chair out, hold the door open for me, pay for me and be nice to me no matter what while treating me like an equal\", 'what if i told you the words \"vegan\" and \"organic\" are not synonymous with the word \"healthy\"', 'Fidget spinner for women. God i hate women like why do they even exist they so useless', \"take a bath they said, it'll be fun they said\", 'thinks religion is nonsense but crystals bring us healing energy', 'Cmon Vanessa, DIG DEEP! You wanna work a 9-5 job or post pics of your ass on Instagram?', 'the ball attacked me, i swear', \"is a girl gamer doesn't try to whore herself out for attention\", \"It's not rape if she really didn't want to, she'd have said something\", \"How girl's look when they take off make up, lashes, eye brows\", \"quick, back in the truck. they've dropped us in Dewsbury\", 'Girls did you know...that uhm, your boobs go inside your shirt', \"i think i've found the shortest work zone ever.. work zone begins. end road work\", 'Demand perfection in men. Hide zits and moles with 3 inches of make-up.', 'If period products oppress women why do women advertise them?', \"before you judge me, please understand that i don't give a fuck whats you think about me\", 'facebook: exists. old white people: this is the perfect place for me to play out my political agenda', 'His and hers', 'when you get three upvotes on a meme that took you two seconds to steal', \"shh.. do you hear that? That's the sound of a sandwicht not being made.\", 'me: opens a package and sets the box aside. my cat:', 'Wait hold up!! Your legs open faster than google home page and you claim that you are wife material?', \"So you're one of those women that think speed walking is a sport..why dont you speedwalk to the fridge and get me a beer\", 'its paid for!', \"you don't post in 'politcs' do you?\", '30% of women should have kust shut the fuck up. More importantly why is this bitch out of the kitchen?', 'when teachers send the test in advance!', 'reddit, anything else. surprised pikachu memes', \"if you wanted to play pc games you shouldn't have bought a mac\", 'The floor is women rights', 'Two types of women in life, choose wisely', 'Annual meeting of women drivers!!', 'Talk about how bad Trump is doing. Take credit for how well Trump is doing', \"cyberbullying?here's one quick solution\", 'women pellets, Respect applicator, me, women. How i respect women.', 'Elizabeth and Hillary representing the nasty women of America for decades. Communist women are very nasty indeed', \"Women drivers. It's the only possible explanation.\", 'Women. Men.', 'jews complining about energy costs? free gas', 'If women want equal pay, they should do equal work. Any job that requires physical labor.', \"Rape culture.  It's what every oxymoronic feminist is wearing this year\", 'walking, running, telereporting, not going to destination', 'taking the time to get her pussy wet. always practice proper rape etiquette', 'what men play with vs what women play with', 'Girls boys school travel sport shopping going to sea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB08m-CqFWlp"
      },
      "source": [
        "image_list = []\n",
        "for i in range (100):\n",
        "  image_list.append(plt.imread(f'Images/{f_name[i]}'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75hvZVrsk7QS"
      },
      "source": [
        "### Load Config and Model Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6g5zOxUlMAe"
      },
      "source": [
        "I am using the MaskRCNN ResNet-101 FPN checkpoint, but you can use any checkpoint of your preference. This checkpoint is pre-trained on the COCO dataset. You can check other checkpoints/configs on the [Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md) page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Sh6otik73l"
      },
      "source": [
        "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "\n",
        "def load_config_and_model_weights(cfg_path):\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
        "\n",
        "    # ROI HEADS SCORE THRESHOLD\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "    # Comment the next line if you're using 'cuda'\n",
        "    cfg['MODEL']['DEVICE']='cpu'\n",
        "\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "cfg = load_config_and_model_weights(cfg_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyzRmh-ogXhZ"
      },
      "source": [
        "### Load the Object Detection Model\n",
        "The `build_model` method can be used to load a model from the configuration, the checkpoints have to be loaded using the `DetetionCheckpointer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJP7gj4bkJao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ca74a7-a417-47e3-8adf-0acf9d00d5b8"
      },
      "source": [
        "def get_model(cfg):\n",
        "    # build model\n",
        "    model = build_model(cfg)\n",
        "\n",
        "    # load weights\n",
        "    checkpointer = DetectionCheckpointer(model)\n",
        "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "    # eval mode\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = get_model(cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_a3ec72.pkl: 254MB [00:11, 22.1MB/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzPVBzpwkr1K"
      },
      "source": [
        "### Convert Image to Model Input\n",
        "The detectron uses resizing and normalization based on the configuration parameters and the input is to be provided using `ImageList`. The `model.backbone.size_divisibility` handles the sizes (padding) such that the FPN lateral and output convolutional features have same dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwPzLa4krkP"
      },
      "source": [
        "def prepare_image_inputs(cfg, img_list):\n",
        "    # Resizing the image according to the configuration\n",
        "    transform_gen = T.ResizeShortestEdge(\n",
        "                # [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
        "                [30, 30], 50\n",
        "            )\n",
        "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
        "\n",
        "    # Convert to C,H,W format\n",
        "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
        "\n",
        "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
        "\n",
        "    # Normalizing the image\n",
        "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
        "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
        "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
        "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
        "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
        "\n",
        "    # Convert to ImageList\n",
        "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
        "    \n",
        "    return images, batched_inputs\n",
        "\n",
        "images, batched_inputs = prepare_image_inputs(cfg, image_list)\n",
        "print(len(images))\n",
        "print(len(images[0]))\n",
        "print(len(images[0][0]))\n",
        "print(len(batched_inputs))\n",
        "print(len(batched_inputs[0]))\n",
        "print(len(batched_inputs[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTpw2nBmcTZ"
      },
      "source": [
        "### Get ResNet+FPN features\n",
        "The ResNet model in combination with FPN generates five features for an image at different levels of complexity. For more details, refer to the FPN paper or this [article](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd). For this tutorial, just know that `p2`, `p3`, `p4`, `p5`, `p6` are the features needed by the RPN (Region Proposal Network). The proposals in combination with `p2`, `p3`, `p4`, `p5` are then used by the ROI (Region of Interest) heads to generate box predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPCSBSYme7c"
      },
      "source": [
        "def get_features(model, images):\n",
        "    features = model.backbone(images.tensor)\n",
        "    return features\n",
        "\n",
        "features = get_features(model, images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCcZa3r2yzVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3a2590-7434-4a9c-f0cb-0df422f0794c"
      },
      "source": [
        "features.keys()\n",
        "print(len(features))\n",
        "print(len(features[0]))\n",
        "print(len(features[0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiP5mpUlml0C"
      },
      "source": [
        "### Get region proposals from RPN\n",
        "This RPN takes in the features and images and generates the proposals. Based on the configuration we chose, we get 1000 proposals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMs_tnKmu8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bcc98c-e71e-460b-e17a-7fb473ef95b1"
      },
      "source": [
        "def get_proposals(model, images, features):\n",
        "    proposals, _ = model.proposal_generator(images, features)\n",
        "    return proposals\n",
        "\n",
        "proposals = get_proposals(model, images, features)\n",
        "print(len(proposals))\n",
        "print(len(proposals[0]))\n",
        "print(len(proposals[0][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "329\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CpoMTE-mxsJ"
      },
      "source": [
        "### Get Box Features for the proposals\n",
        "\n",
        "The proposals and features are then used by the ROI heads to get the predictions. In this case, the partial execution of layers becomes significant. We want the `box_features` to be the `fc2` outputs of the regions. Hence, I use only the layers that are needed until that step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIX9Z7JPN8Y9",
        "outputId": "f6f3e225-88ff-4d65-a116-3ee29e88c054"
      },
      "source": [
        "print(len(image_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uWEQiKMv1a6"
      },
      "source": [
        "box_features = model.roi_heads.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
        "box_features = model.roi_heads.box_head(box_features)\n",
        "predictions = model.roi_heads.box_predictor(box_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjSh0I5mxVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "9bfcc7ca-6502-4666-8117-0bacb9559c95"
      },
      "source": [
        "# the dimensions of box_features determine the end dimensions of the embeddings\n",
        "# if the first argument of reshape() is 100, it will work out\n",
        "def get_box_features(model, features, proposals):\n",
        "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
        "    print(len(features_list), \", \", end=\"\")\n",
        "    print(len(features_list[0]), \", \", end=\"\")\n",
        "    print(len(features_list[0][0]), \", \", end=\"\")\n",
        "    print(len(features_list[0][0][0]), \", \", end=\"\")\n",
        "    print(len(features_list[0][0][0][0]))\n",
        "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "    print(box_features.size())\n",
        "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
        "    print(box_features.size())\n",
        "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
        "    print(box_features.size())\n",
        "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
        "    print(box_features.size())\n",
        "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
        "    print(box_features.size())\n",
        "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
        "\n",
        "    box_features = box_features.reshape(100, 226, 1024) # depends on your config and batch size\n",
        "    return box_features, features_list\n",
        "\n",
        "box_features, features_list = get_box_features(model, features, proposals)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "100\n",
            "256\n",
            "16\n",
            "16\n",
            "torch.Size([22603, 256, 7, 7])\n",
            "torch.Size([22603, 12544])\n",
            "torch.Size([22603, 1024])\n",
            "torch.Size([22603, 1024])\n",
            "torch.Size([22603, 1024])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-ae85c340cf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbox_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbox_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_box_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-ae85c340cf66>\u001b[0m in \u001b[0;36mget_box_features\u001b[0;34m(model, features, proposals)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m226\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# depends on your config and batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbox_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 226, 1024]' is invalid for input of size 23145472"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUX0oBSm5Ia"
      },
      "source": [
        "### Get prediction logits and boxes\n",
        "The prediction class logits and the box predictions from the ROI heads, this is used in the next step to get the boxes and scores from the `FastRCNNOutputs`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtSExOO3m4xN"
      },
      "source": [
        "def get_prediction_logits(model, features_list, proposals):\n",
        "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "    cls_features = model.roi_heads.box_head(cls_features)\n",
        "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
        "    return pred_class_logits, pred_proposal_deltas\n",
        "\n",
        "pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln4M8S27nBsP"
      },
      "source": [
        "### Get FastRCNN scores and boxes\n",
        "\n",
        "This results in the softmax scores and the boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxxAVAbSnBGN"
      },
      "source": [
        "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
        "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
        "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
        "    num_classes = 1\n",
        "    box_head_output_size = 8\n",
        "    predictions = [pred_class_logits, pred_proposal_deltas]\n",
        "\n",
        "    outputs = FastRCNNOutputLayers(\n",
        "      ShapeSpec(channels=box_head_output_size),\n",
        "      box2box_transform=Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS),\n",
        "      num_classes=1,\n",
        "    )\n",
        "\n",
        "\n",
        "    scores = outputs.predict_probs(predictions, proposals)\n",
        "    boxes = outputs.predict_boxes(predictions, proposals)\n",
        "    # image_shapes = outputs.image_shapes\n",
        "\n",
        "    # return boxes, scores, image_shapes\n",
        "    return boxes, scores\n",
        "\n",
        "# boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)\n",
        "boxes, scores = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8706H2CEsqVR"
      },
      "source": [
        "### Rescale the boxes to original image size\n",
        "We want to rescale the boxes to original size as this is done in the detectron2 library. This is done for sanity and to keep it similar to the visualbert repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s3PMb8Asp8d"
      },
      "source": [
        "def get_output_boxes(boxes, batched_inputs, image_size):\n",
        "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
        "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
        "    output_boxes = Boxes(proposal_boxes)\n",
        "\n",
        "    output_boxes.scale(scale_x, scale_y)\n",
        "    output_boxes.clip(image_size)\n",
        "\n",
        "    return output_boxes\n",
        "\n",
        "output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8852e2s0_b"
      },
      "source": [
        "### Select the Boxes using NMS\n",
        "We need two thresholds - NMS threshold for the NMS box section, and score threshold for the score based section.\n",
        "\n",
        "First NMS is performed for all the classes and the max scores of each proposal box and each class is updated.\n",
        "\n",
        "Then the class score threshold is used to select the boxes from those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtuGvVs7V7M-"
      },
      "source": [
        "for i in range (10):\n",
        "  print(len(output_boxes[i])/320)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osFdrPETta9r"
      },
      "source": [
        "def select_boxes(cfg, output_boxes, scores):\n",
        "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
        "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
        "    cls_prob = scores.detach()\n",
        "    dim1 = int(len(output_boxes)/80)\n",
        "    cls_boxes = output_boxes.tensor.detach().reshape(dim1,80,4)\n",
        "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
        "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
        "        cls_scores = cls_prob[:, cls_ind+1]\n",
        "        det_boxes = cls_boxes[:,cls_ind,:]\n",
        "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
        "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
        "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
        "    return keep_boxes, max_conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7eM5r-tV5W"
      },
      "source": [
        "temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
        "keep_boxes, max_conf = [],[]\n",
        "for keep_box, mx_conf in temp:\n",
        "    keep_boxes.append(keep_box)\n",
        "    max_conf.append(mx_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSKwE4Tgth5d"
      },
      "source": [
        "### Limit the total number of boxes\n",
        "In order to get the box features for the best few proposals and limit the sequence length, we set minimum and maximum boxes and pick those box features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAjub3VYtiXK"
      },
      "source": [
        "MIN_BOXES=3\n",
        "MAX_BOXES=4\n",
        "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
        "    if len(keep_boxes) < min_boxes:\n",
        "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
        "    elif len(keep_boxes) > max_boxes:\n",
        "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
        "    return keep_boxes\n",
        "\n",
        "keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoDdhOhctlGK"
      },
      "source": [
        "### Get the visual embeddings :) \n",
        "Finally, the boxes are chosen using the `keep_boxes` indices and from the `box_features` tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIYqr1lWdcPh"
      },
      "source": [
        "zip(box_features, keep_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7iAf-PYd0fl"
      },
      "source": [
        "print(len(box_features))\n",
        "print(len(keep_boxes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W_6YZmaen5Y"
      },
      "source": [
        "print(\"box_features: \")\n",
        "print(len(box_features)) \n",
        "print(len(box_features[0])) \n",
        "print(len(box_features[0][0])) \n",
        "print()\n",
        "print(\"keep boxes:\")\n",
        "print(len(keep_boxes))\n",
        "print(len(keep_boxes[0])) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWc3mkRgXiz"
      },
      "source": [
        "# box_features[0][keep_boxes[0].copy()]\n",
        "kb = keep_boxes[0].copy()\n",
        "# box_features[0][keep_boxes[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzdSv0IWhD1f"
      },
      "source": [
        "box_features[0][kb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yDeAdl_ej2n"
      },
      "source": [
        "def get_visual_embeds(box_features, keep_boxes):\n",
        "    return box_features[keep_boxes.copy()]\n",
        "get_visual_embeds(box_features[0], keep_boxes[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-5rqN-vtlkq"
      },
      "source": [
        "def get_visual_embeds(box_features, keep_boxes):\n",
        "    return box_features[keep_boxes.copy()]\n",
        "\n",
        "# visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
        "visual_embeds = []\n",
        "for box_feature, keep_box in zip(box_features, keep_boxes):\n",
        "  visual_embeds.append(get_visual_embeds(box_feature, keep_box))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfFLAQzpDhuv"
      },
      "source": [
        "\n",
        "print(len(visual_embeds[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnJarwiOdD36"
      },
      "source": [
        "print(len(visual_embeds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH5jfmuH4yuZ"
      },
      "source": [
        "## Tips for putting it all together\n",
        "\n",
        "Note that these methods can be combined into different parts to make it more efficient: \n",
        "1. Get the model and store it in a variable.\n",
        "2. Transform and create batched inputs separately.\n",
        "3. Generate visual embeddings from the detectron on the batched inputs and models.\n",
        "\n",
        "Ideally, you want to build a class around this for ease of use - The class should contain all the methods, the model and the configuration details. And it should process a batch of images and convert to embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLSk85svCziz"
      },
      "source": [
        "## Using the embeddings with VisualBert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odw_QQo7I0cl"
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "# %cd /content/\n",
        "# user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# cmd_string = f'git clone -b add_visualbert --single-branch https://{user}:{password}@github.com/gchhablani/transformers.git'\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# %cd transformers\n",
        "# !pip install -e \".[dev]\"\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdivyuVLC6fq"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertForPreTraining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk2YLidFFHha"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E25fVeCFCz0"
      },
      "source": [
        "questions = [question1, question2]\n",
        "tokens = tokenizer(questions, padding='max_length', max_length=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqsPbWONGhTF"
      },
      "source": [
        "input_ids = torch.tensor(tokens[\"input_ids\"])\n",
        "attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
        "token_type_ids = torch.tensor(tokens[\"token_type_ids\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEB4OP33IOCl"
      },
      "source": [
        "visual_embeds = torch.stack(visual_embeds)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR8eDYeCIhxC"
      },
      "source": [
        "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre') # this checkpoint has 1024 dimensional visual embeddings projection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDdlJxRYIxcT"
      },
      "source": [
        "outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-3P82lYNA3y"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmq8C39meEZX"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Detectron2 Colab Tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)\n",
        "2. [Detectron Repository](https://github.com/facebookresearch/Detectron)\n",
        "3. [Detectron2 Repository](https://github.com/facebookresearch/detectron2)\n",
        "4. [Detectron2 Docs](https://detectron2.readthedocs.io/en/latest/index.html)\n",
        "5. [VisualBert Repository](https://github.com/uclanlp/visualbert)\n",
        "6. [Medium Article on Detectron2 by Hiroto Honda](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd)"
      ]
    }
  ]
}