{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Visual Embeddings using Detectron2 for ðŸ¤— VisualBert  ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeamMAMI/MAMI/blob/VisualBERT/Generating_Visual_Embeddings_using_Detectron2_for_%F0%9F%A4%97_VisualBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qv-4b7LYOVw"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnSobfsQYTEG"
      },
      "source": [
        "*Disclaimer: This notebook is based on my understanding of the [detectron2](https://github.com/facebookresearch/detectron2) and the [visualbert](https://github.com/uclanlp/visualbert) repositories. Hence, I do not guarantee that this is the \"correct\" or \"recommended\" way to get visual embeddings from detectron2. Having said that, I'm definitely looking to improve this notebook and open to any criticism/suggestions. You can reach me at chhablani.gunjan@gmail.com with any issues that concern you regarding this notebook.*\n",
        "\n",
        "This notebook is based on the concept in the [script to extract image features](https://github.com/uclanlp/visualbert/blob/master/utils/get_image_features/extract_image_features_nlvr.py) for NLVR2 task in the [visualbert](https://github.com/uclanlp/visualbert) repository. You can refer to this script for a \"safer\" way to extract visual embeddings. The script uses [detectron](https://github.com/facebookresearch/Detectron) and it'll be fairly easier to use it (I hope) without getting into the nitty-gritty.\n",
        "\n",
        "However, for the sake of using detectron2, which will have better support (for the foreseeable future) than detectron, I present this notebook example to you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY8GzVcva0Xc"
      },
      "source": [
        "For extracting visual embeddings, we need the features from various regions in the image which are used in the classification. This means that we need to \"detect\" the regions which might have objects in them.\n",
        "\n",
        "The detectron2 library, off-the-shelf, does not support intermediate tensor extraction. But, there are ways the user can get the values of these tensors with some effort. See the docs [here](https://detectron2.readthedocs.io/en/latest/tutorials/models.html#partially-execute-a-model). In this notebook, I will be using the *partial execution* method as described in the docs. I admit that the other approaches might be easier or better suited, but this is just a start ;).\n",
        "\n",
        "**Tip:** If you're looking to play with detectron2, you might like this [Colab tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_GB-tubtJs"
      },
      "source": [
        "For the purpose of this notebook, I will be using an example from the [VQA v2](https://visualqa.org/download.html) validation set, as it is one of the tasks VisualBert has been used for. VisualBert authors used pre-generated embeddings for VQA v2, however."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-mLSDb74g58"
      },
      "source": [
        "## How it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Aa4ojk4mTx"
      },
      "source": [
        "The model checkpoint that we will be using for this notebook is a MaskRCNN+ResNet-101+FPN checkpoint.\n",
        "\n",
        "First, the image features are generated at various scales using the ResNet+FPN backbone. These features are then passed to the region proposal network or RPN. RPN generates 1000 region proposals, which are then passed to ROI Heads. ROI Heads perform the classification and box-regression and after that the predictions are aligned using ROIAlign layer and to the mask RCNN heads.\n",
        "\n",
        "We want to extract the box features in the ROI heads which are used for classification. However, we don't want to select all the proposals (as there are 1000 of them!). For the same, we use the NMS with a threshold. Then the boxes are further filtered using a class score threshold. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf-pOxtSfPF0"
      },
      "source": [
        "### Install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "643yOpAZwRWq",
        "outputId": "2922be89-c90c-4547-8c79-2f06430623e1"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKj-udtwT8o",
        "outputId": "4bed8832-e0da-4236-ca17-298c4c4a6c76"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P5ZDpfKXlHX"
      },
      "source": [
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "%%capture\n",
        "!pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG9jjTSOfhX6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xACCRQgLfhLG"
      },
      "source": [
        "import torch, torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from copy import deepcopy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8EUjamjkT6p"
      },
      "source": [
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.structures.image_list import ImageList\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
        "from detectron2.structures.boxes import Boxes\n",
        "from detectron2.layers import nms\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config.config import get_cfg"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLNocZRfThd"
      },
      "source": [
        "### Download the VQA v2 Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZz1sPnHfXmf"
      },
      "source": [
        "# questions\n",
        "%%capture\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip \n",
        "!unzip v2_Questions_Val_mscoco.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZI66WRfYib"
      },
      "source": [
        "# images\n",
        "%%capture\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip val2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8vpHS6fbqd"
      },
      "source": [
        "# answers\n",
        "%%capture\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
        "!unzip v2_Annotations_Val_mscoco.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46stwSuFfpQQ"
      },
      "source": [
        "### Load Examples\n",
        "The next few cells show how to get an example from the VQA v2 dataset. We will only use the image from the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va4s2RC7fuzM"
      },
      "source": [
        "with open('v2_OpenEnded_mscoco_val2014_questions.json') as f:\n",
        "    q = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoJ5FgCJfuqk"
      },
      "source": [
        "with open('v2_mscoco_val2014_annotations.json') as f:\n",
        "    a = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl0-OvilfxoZ"
      },
      "source": [
        "idx = 1500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyAR_t3Uf0sX"
      },
      "source": [
        "question_info = q[\"questions\"][idx]\n",
        "image_id = question_info['image_id']\n",
        "question1 = question_info['question']\n",
        "question_id = question_info['question_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jjbBd_rgECv"
      },
      "source": [
        "answer_info = a['annotations'][idx]\n",
        "answer_word1 = answer_info['multiple_choice_answer']\n",
        "assert question_id == answer_info['question_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kQkNpG6gE-2"
      },
      "source": [
        "img1 = plt.imread(f'val2014/COCO_val2014_{image_id:012d}.jpg')\n",
        "\n",
        "# Detectron expects BGR images\n",
        "img_bgr1 = cv2.cvtColor(img1, cv2.COLOR_RGB2BGR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCrXEiJJgO83"
      },
      "source": [
        "print(img1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Eq5_Y0gTZp"
      },
      "source": [
        "plt.imshow(img1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SECxsxUgUfb"
      },
      "source": [
        "question1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCwNEbUgVBF"
      },
      "source": [
        "answer_word1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePkFZ8Vbevoh"
      },
      "source": [
        "### Taking another image for a \"batch\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBGb_fD8ewOh"
      },
      "source": [
        "idx = 2000\n",
        "\n",
        "question_info = q[\"questions\"][idx]\n",
        "image_id = question_info['image_id']\n",
        "question2 = question_info['question']\n",
        "question_id = question_info['question_id']\n",
        "answer_info = a['annotations'][idx]\n",
        "answer_word2 = answer_info['multiple_choice_answer']\n",
        "\n",
        "img2 = plt.imread(f'val2014/COCO_val2014_{image_id:012d}.jpg')\n",
        "\n",
        "# Detectron expects BGR images\n",
        "img_bgr2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n",
        "plt.imshow(img2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPeorZBe_fkV"
      },
      "source": [
        "img2.shape # Note that images are differently-sized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAaFf3UQ8ogM"
      },
      "source": [
        "question2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_NT2S-H819A"
      },
      "source": [
        "answer_word2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75hvZVrsk7QS"
      },
      "source": [
        "### Load Config and Model Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6g5zOxUlMAe"
      },
      "source": [
        "I am using the MaskRCNN ResNet-101 FPN checkpoint, but you can use any checkpoint of your preference. This checkpoint is pre-trained on the COCO dataset. You can check other checkpoints/configs on the [Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md) page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Sh6otik73l"
      },
      "source": [
        "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "\n",
        "def load_config_and_model_weights(cfg_path):\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
        "\n",
        "    # ROI HEADS SCORE THRESHOLD\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "    # Comment the next line if you're using 'cuda'\n",
        "    cfg['MODEL']['DEVICE']='cpu'\n",
        "\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "cfg = load_config_and_model_weights(cfg_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyzRmh-ogXhZ"
      },
      "source": [
        "### Load the Object Detection Model\n",
        "The `build_model` method can be used to load a model from the configuration, the checkpoints have to be loaded using the `DetetionCheckpointer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJP7gj4bkJao"
      },
      "source": [
        "def get_model(cfg):\n",
        "    # build model\n",
        "    model = build_model(cfg)\n",
        "\n",
        "    # load weights\n",
        "    checkpointer = DetectionCheckpointer(model)\n",
        "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "    # eval mode\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = get_model(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzPVBzpwkr1K"
      },
      "source": [
        "### Convert Image to Model Input\n",
        "The detectron uses resizing and normalization based on the configuration parameters and the input is to be provided using `ImageList`. The `model.backbone.size_divisibility` handles the sizes (padding) such that the FPN lateral and output convolutional features have same dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwPzLa4krkP"
      },
      "source": [
        "def prepare_image_inputs(cfg, img_list):\n",
        "    # Resizing the image according to the configuration\n",
        "    transform_gen = T.ResizeShortestEdge(\n",
        "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
        "            )\n",
        "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
        "\n",
        "    # Convert to C,H,W format\n",
        "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
        "\n",
        "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
        "\n",
        "    # Normalizing the image\n",
        "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
        "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
        "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
        "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
        "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
        "\n",
        "    # Convert to ImageList\n",
        "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
        "    \n",
        "    return images, batched_inputs\n",
        "\n",
        "images, batched_inputs = prepare_image_inputs(cfg, [img_bgr1, img_bgr2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTpw2nBmcTZ"
      },
      "source": [
        "### Get ResNet+FPN features\n",
        "The ResNet model in combination with FPN generates five features for an image at different levels of complexity. For more details, refer to the FPN paper or this [article](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd). For this tutorial, just know that `p2`, `p3`, `p4`, `p5`, `p6` are the features needed by the RPN (Region Proposal Network). The proposals in combination with `p2`, `p3`, `p4`, `p5` are then used by the ROI (Region of Interest) heads to generate box predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPCSBSYme7c"
      },
      "source": [
        "def get_features(model, images):\n",
        "    features = model.backbone(images.tensor)\n",
        "    return features\n",
        "\n",
        "features = get_features(model, images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCcZa3r2yzVk"
      },
      "source": [
        "features.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KPvzqT6mYJu"
      },
      "source": [
        "### Visualizing Image and Image features\n",
        "Just for a sanity check, we visualize the 0th channels in each of the features, and their shapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh7YSy4FmbNx"
      },
      "source": [
        "plt.imshow(cv2.resize(img2, (images.tensor.shape[-2:][::-1])))\n",
        "plt.show()\n",
        "for key in features.keys():\n",
        "    print(features[key].shape)\n",
        "    plt.imshow(features[key][1,0,:,:].squeeze().detach().numpy(), cmap='jet')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiP5mpUlml0C"
      },
      "source": [
        "### Get region proposals from RPN\n",
        "This RPN takes in the features and images and generates the proposals. Based on the configuration we chose, we get 1000 proposals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMs_tnKmu8G"
      },
      "source": [
        "def get_proposals(model, images, features):\n",
        "    proposals, _ = model.proposal_generator(images, features)\n",
        "    return proposals\n",
        "\n",
        "proposals = get_proposals(model, images, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CpoMTE-mxsJ"
      },
      "source": [
        "### Get Box Features for the proposals\n",
        "\n",
        "The proposals and features are then used by the ROI heads to get the predictions. In this case, the partial execution of layers becomes significant. We want the `box_features` to be the `fc2` outputs of the regions. Hence, I use only the layers that are needed until that step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjSh0I5mxVE"
      },
      "source": [
        "def get_box_features(model, features, proposals):\n",
        "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
        "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
        "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
        "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
        "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
        "\n",
        "    box_features = box_features.reshape(2, 1000, 1024) # depends on your config and batch size\n",
        "    return box_features, features_list\n",
        "\n",
        "box_features, features_list = get_box_features(model, features, proposals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUX0oBSm5Ia"
      },
      "source": [
        "### Get prediction logits and boxes\n",
        "The prediction class logits and the box predictions from the ROI heads, this is used in the next step to get the boxes and scores from the `FastRCNNOutputs`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtSExOO3m4xN"
      },
      "source": [
        "def get_prediction_logits(model, features_list, proposals):\n",
        "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "    cls_features = model.roi_heads.box_head(cls_features)\n",
        "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
        "    return pred_class_logits, pred_proposal_deltas\n",
        "\n",
        "pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln4M8S27nBsP"
      },
      "source": [
        "### Get FastRCNN scores and boxes\n",
        "\n",
        "This results in the softmax scores and the boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxxAVAbSnBGN"
      },
      "source": [
        "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
        "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
        "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
        "\n",
        "    outputs = FastRCNNOutputLayers(\n",
        "        box2box_transform,\n",
        "        pred_class_logits,\n",
        "        pred_proposal_deltas,\n",
        "        proposals,\n",
        "        smooth_l1_beta,\n",
        "    )\n",
        "\n",
        "    boxes = outputs.predict_boxes()\n",
        "    scores = outputs.predict_probs()\n",
        "    image_shapes = outputs.image_shapes\n",
        "\n",
        "    return boxes, scores, image_shapes\n",
        "\n",
        "boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm7VAbhz1H7P"
      },
      "source": [
        "boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8706H2CEsqVR"
      },
      "source": [
        "### Rescale the boxes to original image size\n",
        "We want to rescale the boxes to original size as this is done in the detectron2 library. This is done for sanity and to keep it similar to the visualbert repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s3PMb8Asp8d"
      },
      "source": [
        "def get_output_boxes(boxes, batched_inputs, image_size):\n",
        "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
        "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
        "    output_boxes = Boxes(proposal_boxes)\n",
        "\n",
        "    output_boxes.scale(scale_x, scale_y)\n",
        "    output_boxes.clip(image_size)\n",
        "\n",
        "    return output_boxes\n",
        "\n",
        "output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8852e2s0_b"
      },
      "source": [
        "### Select the Boxes using NMS\n",
        "We need two thresholds - NMS threshold for the NMS box section, and score threshold for the score based section.\n",
        "\n",
        "First NMS is performed for all the classes and the max scores of each proposal box and each class is updated.\n",
        "\n",
        "Then the class score threshold is used to select the boxes from those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osFdrPETta9r"
      },
      "source": [
        "def select_boxes(cfg, output_boxes, scores):\n",
        "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
        "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
        "    cls_prob = scores.detach()\n",
        "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
        "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
        "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
        "        cls_scores = cls_prob[:, cls_ind+1]\n",
        "        det_boxes = cls_boxes[:,cls_ind,:]\n",
        "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
        "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
        "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
        "    return keep_boxes, max_conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7eM5r-tV5W"
      },
      "source": [
        "temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
        "keep_boxes, max_conf = [],[]\n",
        "for keep_box, mx_conf in temp:\n",
        "    keep_boxes.append(keep_box)\n",
        "    max_conf.append(mx_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSKwE4Tgth5d"
      },
      "source": [
        "### Limit the total number of boxes\n",
        "In order to get the box features for the best few proposals and limit the sequence length, we set minimum and maximum boxes and pick those box features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAjub3VYtiXK"
      },
      "source": [
        "MIN_BOXES=10\n",
        "MAX_BOXES=100\n",
        "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
        "    if len(keep_boxes) < min_boxes:\n",
        "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
        "    elif len(keep_boxes) > max_boxes:\n",
        "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
        "    return keep_boxes\n",
        "\n",
        "keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoDdhOhctlGK"
      },
      "source": [
        "### Get the visual embeddings :) \n",
        "Finally, the boxes are chosen using the `keep_boxes` indices and from the `box_features` tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-5rqN-vtlkq"
      },
      "source": [
        "def get_visual_embeds(box_features, keep_boxes):\n",
        "    return box_features[keep_boxes.copy()]\n",
        "\n",
        "visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH5jfmuH4yuZ"
      },
      "source": [
        "## Tips for putting it all together\n",
        "\n",
        "Note that these methods can be combined into different parts to make it more efficient: \n",
        "1. Get the model and store it in a variable.\n",
        "2. Transform and create batched inputs separately.\n",
        "3. Generate visual embeddings from the detectron on the batched inputs and models.\n",
        "\n",
        "Ideally, you want to build a class around this for ease of use - The class should contain all the methods, the model and the configuration details. And it should process a batch of images and convert to embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLSk85svCziz"
      },
      "source": [
        "## Using the embeddings with VisualBert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odw_QQo7I0cl"
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "# %cd /content/\n",
        "# user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# cmd_string = f'git clone -b add_visualbert --single-branch https://{user}:{password}@github.com/gchhablani/transformers.git'\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# %cd transformers\n",
        "# !pip install -e \".[dev]\"\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdivyuVLC6fq"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertForPreTraining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk2YLidFFHha"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E25fVeCFCz0"
      },
      "source": [
        "questions = [question1, question2]\n",
        "tokens = tokenizer(questions, padding='max_length', max_length=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqsPbWONGhTF"
      },
      "source": [
        "input_ids = torch.tensor(tokens[\"input_ids\"])\n",
        "attention_mask = torch.tensor(tokens[\"attention_mask\"])\n",
        "token_type_ids = torch.tensor(tokens[\"token_type_ids\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEB4OP33IOCl"
      },
      "source": [
        "visual_embeds = torch.stack(visual_embeds)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR8eDYeCIhxC"
      },
      "source": [
        "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre') # this checkpoint has 1024 dimensional visual embeddings projection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDdlJxRYIxcT"
      },
      "source": [
        "outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-3P82lYNA3y"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmq8C39meEZX"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Detectron2 Colab Tutorial](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=h9tECBQCvMv3)\n",
        "2. [Detectron Repository](https://github.com/facebookresearch/Detectron)\n",
        "3. [Detectron2 Repository](https://github.com/facebookresearch/detectron2)\n",
        "4. [Detectron2 Docs](https://detectron2.readthedocs.io/en/latest/index.html)\n",
        "5. [VisualBert Repository](https://github.com/uclanlp/visualbert)\n",
        "6. [Medium Article on Detectron2 by Hiroto Honda](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd)"
      ]
    }
  ]
}